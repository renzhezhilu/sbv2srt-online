1
00:00:01,990 --> 00:00:03,368
Neural Networks are good
神经网络有助于学

2
00:00:03,368 --> 00:00:05,370
for learning lots of different types of patterns.
习许多不同类型的模式。

3
00:00:07,000 --> 00:00:08,952
To give an example of how is it work
举个例子来说明它是如何工作的

4
00:00:08,952 --> 00:00:11,100
imagine you had a four pixel camera.
，假设你有一个四像素的摄像头。

5
00:00:11,500 --> 00:00:14,638
So not four megapixels, but just four pixels.
所以不是四百万像素，而是四个像素。

6
00:00:14,638 --> 00:00:16,522
And it was only black and white.
只有黑白两色。

7
00:00:17,219 --> 00:00:19,749
And you wanted to go around and take pictures of things
你想到处走走，拍拍照片

8
00:00:19,749 --> 00:00:22,813
and determine automatically then
然后自动确定

9
00:00:22,813 --> 00:00:24,704
whether these pictures were
这些照片是否

10
00:00:25,484 --> 00:00:30,587
solid, all white or all dark image, vertical line
实心的，全白或全黑的图像，垂直线

11
00:00:30,587 --> 00:00:34,027
or a diagonal line or a horizontal line?
或者一条对角线或者一条水平线？

12
00:00:35,569 --> 00:00:39,372
This is tricky because you can't do this with simple rules
这是棘手的，因为你不能这样做的简单规

13
00:00:39,372 --> 00:00:41,203
about the brightness of the pixels.
则的像素的亮度。

14
00:00:41,636 --> 00:00:43,928
Both of these are horizontal lines,
这两条都是水平线,

15
00:00:43,928 --> 00:00:47,913
but if you try to make a rule about which pixel was bright
但是，如果你试图制定一个规则，哪个像素是明亮的，

16
00:00:47,913 --> 00:00:50,702
and which was dark you wouldn't be able to do it.
哪个是暗的，你就不能这样做。

17
00:00:53,469 --> 00:00:55,556
So to do this with the neural network
所以要用神经网络做到这一点，

18
00:00:55,556 --> 00:00:57,389
you start by taking all of your inputs,
你首先要获取你所有的输入,

19
00:00:57,389 --> 00:00:58,848
in this case our four pixels,
在这个例子中，我们的四个像素,

20
00:00:58,848 --> 00:01:01,480
and you break them out into input neurons.
把它们分解成输入神经元。

21
00:01:02,288 --> 00:01:04,562
You assign a number to each of these
根据像素的亮度或暗度，

22
00:01:04,562 --> 00:01:07,996
depending on the brightness or darkness of the pixel.
为每个像素分配一个数字。

23
00:01:08,380 --> 00:01:12,360
+1 is all the way white. -1 is all the way black.
+ 1是所有的白色-1是所有的黑色。

24
00:01:12,360 --> 00:01:15,194
And then gray is zero right in the middle.
中间的灰色为零。

25
00:01:17,380 --> 00:01:20,200
So these values once you have them broken out
一旦你把这些值分解出来，像这

26
00:01:20,200 --> 00:01:22,368
and listed like this on the input neurons,
样列在输入神经元上,

27
00:01:22,368 --> 00:01:25,623
It's also called the input vector or array.
它也被称为输入向量或数组。

28
00:01:25,623 --> 00:01:27,487
It's just a list of numbers
它只是一个代

29
00:01:27,487 --> 00:01:31,285
that represents your inputs right now.
表你现在输入的数字列表。

30
00:01:33,591 --> 00:01:36,425
It's a useful notion to think about
考虑神经元的接收区

31
00:01:36,425 --> 00:01:38,971
the receptive field of a neuron.
是一个有用的概念。

32
00:01:40,227 --> 00:01:43,634
All this means is what set of inputs
所有这一切意味着什么样的输入

33
00:01:43,634 --> 00:01:47,400
makes the value of this neuron as high as it can possibly be.
设置使这个神经元的价值尽可能高。

34
00:01:48,250 --> 00:01:51,123
For input neurons this is pretty easy.
对于输入神经元来说，这是相当容易的。

35
00:01:51,504 --> 00:01:54,443
Each one is associated with just one pixel
每个神经元只与一个像素相

36
00:01:54,443 --> 00:01:58,006
and when that pixel is all the way white
关联，当这个像素全部为白色时

37
00:01:58,006 --> 00:02:02,053
the value of that input neuron is as high as it could go.
，输入神经元的值就会达到最高值。

38
00:02:02,799 --> 00:02:05,568
The black and white checkered areas show pixels
黑白方格区域显示输入

39
00:02:05,568 --> 00:02:08,523
that an input neuron doesn't care about.
神经元不关心的像素。

40
00:02:08,523 --> 00:02:10,778
If they're all the way white or all the way black
如果它们全部都是白色或者

41
00:02:10,778 --> 00:02:14,530
it still doesn't affect the value of that input on it all.
全部都是黑色，它仍然不会影响所有输入的值。

42
00:02:16,870 --> 00:02:22,065
Now to build a neural network we create a neuron.
现在，为了构建一个神经网络，我们创建了一个神经元。

43
00:02:22,270 --> 00:02:24,839
The first thing this does is it adds up
这样做的第一件事就是

44
00:02:24,839 --> 00:02:28,277
all of the values of the input neurons.
把所有输入神经元的值加起来。

45
00:02:29,680 --> 00:02:33,640
So in this case if we add up all of those values we get a 0.50
所以在这种情况下，如果我们把所有这些值加起来，我们得到0.50

46
00:02:35,438 --> 00:02:37,515
Now to complicate things just a little bit
现在把事情复杂化

47
00:02:38,184 --> 00:02:41,555
each of the connections are weighted
一点，每个连接都是加权的，

48
00:02:41,555 --> 00:02:44,430
meaning they're multiplied by a number.
意思是它们被乘以一个数。

49
00:02:44,430 --> 00:02:49,453
That number can be 1 or -1 or anything in between.
这个数字可以是1或者 -1，或者介于两者之间的任何数字。

50
00:02:49,810 --> 00:02:52,664
So for instance if something has a weight of -1
举个例子，如果某个物体的

51
00:02:52,664 --> 00:02:56,203
its multiplied and you get the negative of it.
重量是-1，它就会乘以它的负数。

52
00:02:56,203 --> 00:02:57,343
And that's added in.
而且还要加进去。

53
00:02:57,343 --> 00:02:59,081
If something has a weight of zero
如果某物的重量为零，

54
00:02:59,081 --> 00:03:00,914
then it's effectively ignored.
那么它就被有效地忽略了。

55
00:03:01,900 --> 00:03:05,084
So here's what those weighted connections might look like.
这就是这些加权连接可能的样子。

56
00:03:05,170 --> 00:03:08,840
You'll notice that after the values of the input neurons
你会注意到，在输入神经元的值被加权并

57
00:03:08,840 --> 00:03:10,939
are weighted and added
且加上这些值之后..

58
00:03:10,939 --> 00:03:14,344
the values come... the final value is completely different.
. ... 最终的值是完全不同的。

59
00:03:18,340 --> 00:03:20,851
Graphically it's convenient to represent these weights
用图形来表示这些权重是很方便的

60
00:03:20,851 --> 00:03:24,729
as white links being positive weights
白色链接是正权重

61
00:03:24,729 --> 00:03:26,951
black links being negative weights
黑色链接是负重量

62
00:03:26,951 --> 00:03:28,930
and the thickness of the line
还有线条的粗细

63
00:03:28,930 --> 00:03:32,550
is roughly proportional to the magnitude of the weight.
大致与重量成正比。

64
00:03:35,736 --> 00:03:39,981
Then after you add the weighted input neurons
然后加上加权输入神经元，它们就被压

65
00:03:40,209 --> 00:03:44,358
they get squashed and I'll show you what that means.
扁了，我会向你们展示这意味着什么。

66
00:03:44,530 --> 00:03:47,648
You have a sigmoid squashing function.
你有一个 s 形挤压函数。

67
00:03:47,648 --> 00:03:50,161
Sigmoid just means S-shaped.
S 形的意思就是 s 形。

68
00:03:50,161 --> 00:03:54,328
And what this does is you put a value in,
这样做的目的就是把一个值放进去,

69
00:03:54,328 --> 00:03:55,630
let's say 0.5
就算0.5吧

70
00:03:56,659 --> 00:04:00,018
And you run a vertical line up to your sigmoid
然后你沿着一条直线跑到你的乙状结肠

71
00:04:00,018 --> 00:04:02,205
and then a horizontal line over
然后是一条水平线

72
00:04:02,205 --> 00:04:03,616
from where it crosses
从它穿过的地方

73
00:04:03,616 --> 00:04:06,512
and then where that hits the Y-axis
然后到达 y 轴

74
00:04:06,512 --> 00:04:08,183
that's the output of your function.
这就是你功能的输出。

75
00:04:08,183 --> 00:04:10,882
So in this case slightly less than point five.
所以在这个例子中略低于0.5。

76
00:04:10,882 --> 00:04:12,177
It's pretty close.
很接近了。

77
00:04:13,750 --> 00:04:16,388
As your input number gets larger
随着输入数量的增加，

78
00:04:16,388 --> 00:04:19,818
output number also gets larger, but more slowly.
输出数量也会增加，但速度会变慢。

79
00:04:19,818 --> 00:04:21,061
And eventually,
最终,

80
00:04:21,061 --> 00:04:23,606
no matter how big the number you put in
不管你输入的数字

81
00:04:23,606 --> 00:04:27,206
the answer is always less than one.
有多大，答案总是小于1。

82
00:04:27,206 --> 00:04:29,369
Similarly when you go negative
同样，当你消极的时候

83
00:04:29,369 --> 00:04:31,664
the answer is always greater than negative one.
，答案总是大于消极的。

84
00:04:32,560 --> 00:04:35,904
So this ensures that neurons value
因此，这保证了神经元的

85
00:04:36,280 --> 00:04:40,065
never gets outside of the range of +1 to -1
值永远不会超出 + 1到

86
00:04:40,065 --> 00:04:44,935
which is helpful for keeping the computations
-1的范围，这有助于保持神经网

87
00:04:44,935 --> 00:04:48,444
in the neural network bounded and stable.
络中的计算有界和稳定。

88
00:04:50,710 --> 00:04:54,186
So after you sum the weighted values of neurons
所以当你把神经元的加权值加起来

89
00:04:54,186 --> 00:04:56,608
squash the result -- you get the output.
之后，你就得到了输出。

90
00:04:56,608 --> 00:04:59,080
In this case 0.746
在这种情况下，

91
00:04:59,080 --> 00:05:01,649
That is a neuron.
这是一个神经元。

92
00:05:01,810 --> 00:05:03,152
So we can call this...
所以我们可以称之为..。

93
00:05:03,152 --> 00:05:06,101
we can collapse all that down and this is a neuron
我们可以把这些都倒下，这是一个做加权和

94
00:05:06,101 --> 00:05:08,351
that does a weighted sum and squash the result.
运算的神经元，然后压缩结果。

95
00:05:09,130 --> 00:05:12,341
And now instead of just one of those
现在不再是只有一个假设你

96
00:05:12,370 --> 00:05:14,144
assume you have a whole bunch
有一大堆，

97
00:05:14,144 --> 00:05:19,378
there are 4 shown here, but there could be 400 or 4 million.
而是有4个，但是可能有400或400万。

98
00:05:20,530 --> 00:05:23,881
Now to keep our picture clear we'll assume for now
现在为了让我们的图片更清晰，我们假

99
00:05:23,881 --> 00:05:27,184
that the weights are either +1, white lines,
设现在的权重要么是 + 1，白线,

100
00:05:27,184 --> 00:05:29,648
-1, black lines,
- 1，黑线,

101
00:05:29,648 --> 00:05:32,962
or 0, which case they're missing entirely.
或者0，这种情况下，他们完全失踪。

102
00:05:32,962 --> 00:05:37,555
But in actuality all of these neurons that we created
但实际上，我们创造的所有这些神经元，

103
00:05:37,555 --> 00:05:41,411
are each attached to all of the input neurons
都附着在所有的输入神经元上，

104
00:05:41,411 --> 00:05:45,852
and they all have some weight between -1 and +1.
它们的权重都在 -1和 + 1之间。

105
00:05:48,820 --> 00:05:52,649
When we create this first layer of our neural network
当我们创建神经网络的第一层时

106
00:05:52,649 --> 00:05:55,568
the receptive fields get more complex.
，感受域变得更加复杂。

107
00:05:55,570 --> 00:05:57,986
For instance here each of those
例如，这里每个神经元最终结合

108
00:05:57,986 --> 00:06:01,111
end up combining two of our input neurons
了我们的两个输入神经元，所以这个值.

109
00:06:01,570 --> 00:06:04,613
and so the value... the receptive field...
.. ... 接收区... ..。

110
00:06:04,613 --> 00:06:09,842
the pixel values that make that first layer neuron,
构成第一层神经元的像素值,

111
00:06:09,842 --> 00:06:12,801
as large as it can possibly be,
尽可能的大,

112
00:06:12,801 --> 00:06:15,659
look now like pairs of pixels.
现在看起来像一对对的像素。

113
00:06:15,670 --> 00:06:20,301
Either all white or a mixture of white and black
根据重量的不同，可以是全白的，也可

114
00:06:20,680 --> 00:06:23,044
depending on the weights.
以是黑白混合的。

115
00:06:24,640 --> 00:06:27,906
So for instance this neuron here
例如，这个神经元连接到

116
00:06:28,883 --> 00:06:33,456
is attached to this input pixel (which is upper-left)
这个输入像素(左上角)和这个输入

117
00:06:33,456 --> 00:06:36,379
and this input pixel (just lower left)
像素(左下角) ，

118
00:06:36,379 --> 00:06:38,774
and both of those weights are positive.
这两个权重都是正值。

119
00:06:38,860 --> 00:06:41,445
So it combines the two of those.
所以它结合了这两者。

120
00:06:41,445 --> 00:06:42,996
And that's it's receptive field.
这是一个接受的领域。

121
00:06:42,996 --> 00:06:44,325
The receptive field of this one
这一个的感受野加

122
00:06:44,325 --> 00:06:45,812
plus the receptive field of this one.
上这一个的感受野。

123
00:06:47,080 --> 00:06:48,954
However, if we look at this neuron
然而，如果我们观

124
00:06:49,368 --> 00:06:52,741
it combines...
察这个神经元，它结合了..。

125
00:06:53,471 --> 00:06:58,212
this pixel upper right and this pixel lower right.
右上角的像素，右下角的像素。

126
00:06:58,212 --> 00:07:02,088
It has a weight of minus one for the lower right pixel
右下角的像素的重量为负一，这意

127
00:07:02,088 --> 00:07:07,118
so that means it's most active when this pixel is black,
味着当这个像素是黑色时，它是最活跃的,

128
00:07:07,118 --> 00:07:09,331
so here is its receptive field.
这里是它的感受野。

129
00:07:12,016 --> 00:07:15,757
Now because we were careful
现在，因为我们小心地创

130
00:07:15,757 --> 00:07:18,070
of how we created that first layer
建了第一层，

131
00:07:18,070 --> 00:07:21,802
its values look a lot like input values.
它的值看起来很像输入值。

132
00:07:21,802 --> 00:07:25,416
And we can turn right around and create another layer
我们可以右转，在上面再创建一个

133
00:07:25,416 --> 00:07:27,960
on top of it the exact same way
图层，同样的方式，

134
00:07:27,960 --> 00:07:32,156
with the output of one layer being the input to the next layer.
一个图层的输出就是下一个图层的输入。

135
00:07:32,156 --> 00:07:36,733
And we can repeat this three times or seven times
我们可以重复这个过程三次，七次

136
00:07:36,733 --> 00:07:39,324
or 700 times for additional layers.
，或者700次。

137
00:07:41,344 --> 00:07:44,762
Each time the receptive fields get even more complex.
每次感受野变得更加复杂。

138
00:07:44,762 --> 00:07:47,493
So you can see here using the same logic
所以你可以看到这里使用相

139
00:07:47,493 --> 00:07:50,700
now they cover all of the pixels
同的逻辑现在他们覆盖了所有的

140
00:07:51,429 --> 00:07:54,559
and more special arrangement
像素和更多的特殊安排，其中

141
00:07:54,559 --> 00:07:56,430
of which are black and which are white.
是黑色和白色的。

142
00:07:59,529 --> 00:08:01,529
We can create another layer...
我们可以创建另一个图层..。

143
00:08:02,829 --> 00:08:06,588
Again all of these neurons in one layer are connected
同样，在一层的所有这些神经元都连接到前

144
00:08:06,588 --> 00:08:08,460
to all of the neurons in the previous layer.
一层的所有神经元。

145
00:08:08,800 --> 00:08:10,330
But we're assuming here
但是我们在这里假设大部分

146
00:08:10,330 --> 00:08:12,510
that most of those weights are zero and not shown.
的权重都是零，并且没有显示出来。

147
00:08:13,089 --> 00:08:15,089
It's not generally the case.
事实并非如此。

148
00:08:16,737 --> 00:08:19,149
So just to mix things up, we'll create a new layer,
所以为了混合起来，我们要创建一个新的层,

149
00:08:19,149 --> 00:08:22,110
but if you notice our squashing function isn't there anymore.
但是如果你注意到我们的挤压功能已经不存在了。

150
00:08:22,499 --> 00:08:26,240
We have something new called a rectified linear unit.
我们有一个新的东西，叫做整流线性单位。

151
00:08:26,725 --> 00:08:30,870
This is another popular neuron type.
这是另一种流行的神经元类型。

152
00:08:30,870 --> 00:08:34,227
So you do your weighted sum of all your inputs
所以你做你所有输入的加

153
00:08:34,227 --> 00:08:39,359
and instead of squashing you do rectified linear units.
权和，而不是挤压你做整流线性单位。

154
00:08:40,060 --> 00:08:41,374
You rectify it.
你去纠正它。

155
00:08:41,374 --> 00:08:45,512
So if it is negative you make the value 0.
所以如果是负数，那么值就是0。

156
00:08:45,512 --> 00:08:47,339
If it's positive you keep the value.
如果它是正值，你就保持它的值。

157
00:08:48,699 --> 00:08:52,555
This is obviously very easy to compute
这显然是非常容易计算，并证明

158
00:08:52,555 --> 00:08:56,985
and it turns out to have very nice stability properties
它有非常好的稳定性属性，为神经网

159
00:08:56,985 --> 00:08:58,284
for neural networks as well.
络以及。

160
00:08:58,284 --> 00:08:58,831
In practice.
实际上。

161
00:09:01,569 --> 00:09:05,335
So after we do this because some of our weights are positive
因为有些重量是正的，有些

162
00:09:05,335 --> 00:09:06,609
and some are negative,
是负的,

163
00:09:06,609 --> 00:09:08,865
connecting to those rectified linear units,
连接到那些校正的线性单位,

164
00:09:08,865 --> 00:09:12,838
we get receptive fields and their opposites.
我们有接受能量场和它们的对立面。

165
00:09:12,838 --> 00:09:15,300
Look at the patterns there.
看看这些图案。

166
00:09:16,809 --> 00:09:19,641
And then finally when we've created as many layers
最后，当我们创建了尽可能多的

167
00:09:19,641 --> 00:09:21,318
with as many neurons as we want
神经元层，我们创

168
00:09:21,318 --> 00:09:22,845
we create an output layer.
建了一个输出层。

169
00:09:23,824 --> 00:09:26,631
Here we have four outputs that we're interested in.
这里我们有四个我们感兴趣的输出。

170
00:09:26,631 --> 00:09:32,202
Is the image solid, vertical, diagonal or horizontal.
图像是实心的、垂直的、对角线的还是水平的。

171
00:09:34,660 --> 00:09:37,078
So to walk through an example here
我们来看一个

172
00:09:37,078 --> 00:09:38,044
of how this would work,
例子,

173
00:09:38,044 --> 00:09:40,779
let's say we start with this input image
假设我们从左边的输入

174
00:09:41,580 --> 00:09:42,782
shown on the left.
图像开始。

175
00:09:43,600 --> 00:09:46,281
Dark pixels on top, white on the bottom.
上面是黑色的像素，下面是白色的。

176
00:09:46,751 --> 00:09:49,953
As we propagate that to our input layer
当我们把它传播到我们的输入层时，

177
00:09:49,953 --> 00:09:52,747
this is what those values would look like.
这就是那些值的样子。

178
00:09:53,378 --> 00:09:55,555
The top pixels, the bottom pixels.
顶部像素，底部像素。

179
00:09:56,290 --> 00:09:59,360
As we move that to our first layer
当我们移动到我们的第一层，

180
00:09:59,800 --> 00:10:03,777
we can see the combination of a dark pixel
我们可以看到一个黑色像素和一个光

181
00:10:04,897 --> 00:10:07,494
and a light pixel summed together
色像素的组合在一起得到

182
00:10:07,494 --> 00:10:09,882
get us zero -- gray.
我们的零——灰色。

183
00:10:11,319 --> 00:10:13,022
Whereas down here we have
然而在这里，

184
00:10:13,022 --> 00:10:15,230
the combination of a dark pixel
我们有一个黑色像素和

185
00:10:15,230 --> 00:10:18,379
plus a light pixel with a negative weight.
一个负重量的亮像素的组合。

186
00:10:18,555 --> 00:10:22,194
So that gets us a value of negative one here.
所以这里我们得到了一个负1的值。

187
00:10:22,194 --> 00:10:23,038
Which makes sense
这是有道理的

188
00:10:23,038 --> 00:10:25,784
because if we look at the receptive field here...
，因为如果我们看看这里的感受域..。

189
00:10:26,679 --> 00:10:28,157
Upper left pixel white,
左上角白色像素,

190
00:10:28,157 --> 00:10:30,328
lower left pixel black.
左下黑色像素。

191
00:10:30,328 --> 00:10:34,630
This the exact opposite of the input that we're getting
这与我们得到的输入正好相反，所以

192
00:10:34,630 --> 00:10:38,333
and so we would expect its value to be as low as possible.
我们希望它的值尽可能的低。

193
00:10:38,439 --> 00:10:39,486
Minus one.
减去一。

194
00:10:42,160 --> 00:10:43,965
As we move to the next layer
当我们移动到下一层，

195
00:10:43,965 --> 00:10:45,736
we see the same types of things.
我们看到相同类型的东西。

196
00:10:45,736 --> 00:10:48,738
Combining zeros to get zeros.
组合零得到零。

197
00:10:50,199 --> 00:10:54,514
Combining a negative and a negative with a negative weight
把一个负的、一个负的和一个负的重量结合起来

198
00:10:54,514 --> 00:10:57,303
which makes a positive to get a zero.
，使得一个正的得到一个零。

199
00:10:57,892 --> 00:11:01,161
And here we have combining two negatives to get a negative.
这里我们把两个负数结合起来得到一个负数。

200
00:11:01,360 --> 00:11:04,225
So again you'll notice the receptive field of this
所以你们会再次注意到，这个接受

201
00:11:04,225 --> 00:11:06,626
is exactly the inverse of our input.
域正好与我们的输入相反。

202
00:11:06,626 --> 00:11:10,143
So it makes sense that it's weight would be negative.
所以它的重量是负值是有道理的。

203
00:11:11,132 --> 00:11:12,741
Or its value would be negative.
否则它的价值将是负数。

204
00:11:13,930 --> 00:11:16,299
And we move to the next layer.
然后我们进入下一层。

205
00:11:17,111 --> 00:11:20,358
All of these, of course, these zeros propagate forward.
当然，所有这些零都是向前传播的。

206
00:11:21,160 --> 00:11:22,181
Here...
给你。

207
00:11:22,390 --> 00:11:25,561
This is a negative has a negative value,
这是一个负值,

208
00:11:25,561 --> 00:11:27,921
and it gets... has a positive weight.
它的重量是正的。

209
00:11:27,921 --> 00:11:29,447
So it just moves straight forward.
所以它只是直接向前移动。

210
00:11:29,447 --> 00:11:31,832
Because we have a rectified linear unit,
因为我们有一个矫正的线性单位,

211
00:11:31,832 --> 00:11:34,313
negative values become zero.
负值变成零。

212
00:11:34,313 --> 00:11:36,751
So now it is zero again too.
所以现在它又是零了。

213
00:11:36,751 --> 00:11:39,450
But this one gets rectified and becomes positive.
但是这一点得到纠正，变得积极。

214
00:11:39,450 --> 00:11:41,354
Negative times a negative is positive.
负数乘以负数就是正数。

215
00:11:41,354 --> 00:11:44,021
And so when we finally get to the output
所以当我们最终得到输出时，

216
00:11:44,021 --> 00:11:47,505
we can see they're all zero except for this horizontal,
我们可以看到除了这个水平面，它们都是零,

217
00:11:47,505 --> 00:11:49,954
which is positive, and that's the answer.
这是积极的，这就是答案。

218
00:11:49,954 --> 00:11:51,608
Our neural network said
我们的神经网

219
00:11:51,608 --> 00:11:55,198
this is an image of a horizontal line.
络说这是一条水平线的图像。

220
00:11:57,400 --> 00:12:00,639
Now, neural networks usually aren't that good,
神经网络通常不是那么好,

221
00:12:00,639 --> 00:12:01,831
not that clean.
没那么干净。

222
00:12:01,870 --> 00:12:06,386
So there's a notion of with an input what is truth?
所以输入的概念是什么是真理？ 

223
00:12:06,386 --> 00:12:08,119
In this case the truth is
在这个例子中

224
00:12:08,119 --> 00:12:10,996
this has a zero for all of these values,
，所有这些值都是0,

225
00:12:10,996 --> 00:12:13,758
but a one for horizontal.
而是水平方向的。

226
00:12:13,758 --> 00:12:16,261
It's not solid. It's not vertical. It's not diagonal.
它不是实心的，不是垂直的，不是对角线的。

227
00:12:16,580 --> 00:12:18,097
Yes, it is horizontal.
是的，它是水平的。

228
00:12:19,420 --> 00:12:23,146
An arbitrary neural network will give answers
任意的神经网络会给出不完全正

229
00:12:23,146 --> 00:12:24,581
that are not exactly truth.
确的答案。

230
00:12:24,581 --> 00:12:26,906
It might be off by a little or a lot.
它可能会偏离一点或者很多。

231
00:12:26,906 --> 00:12:32,116
And then the error is the magnitude of the difference
然后这个错误就是真相和给出的答

232
00:12:32,116 --> 00:12:34,370
between the truth and the answer given.
案之间的差距。

233
00:12:34,370 --> 00:12:36,210
And you can add all these up
你可以把所有这些加

234
00:12:36,210 --> 00:12:38,884
to get the total error for the neural network.
起来，得到神经网络的总误差。

235
00:12:40,600 --> 00:12:41,959
So the idea...
所以这个想法..。

236
00:12:41,959 --> 00:12:45,204
the whole idea with learning and training
学习和训练的整个思

237
00:12:45,204 --> 00:12:47,430
is to adjust the weights
想就是调整权

238
00:12:47,430 --> 00:12:50,762
to make the error as low as possible.
重使误差尽可能小。

239
00:12:51,849 --> 00:12:53,978
So the way this is done is...
所以做这件事的方法是..。

240
00:12:53,978 --> 00:12:57,745
Put an image in we calculate the error at the end,
把一个图像放进去，我们计算最后的误差,

241
00:12:57,745 --> 00:13:00,963
then we look for how to adjust those weights
然后我们寻找如何调整这

242
00:13:00,963 --> 00:13:05,786
higher or lower to either make that error go up or down.
些权重或高或低，使误差上升或下降。

243
00:13:05,786 --> 00:13:08,046
And we of course adjust the weights in the way
我们当然要调整权重，而不

244
00:13:08,046 --> 00:13:09,241
than make the error go down.
是降低误差。

245
00:13:10,870 --> 00:13:13,918
Now, the problem with doing this
现在，这样做的问题是，

246
00:13:13,918 --> 00:13:17,220
is each time we go back and calculate the error
每次我们回去计算误差，

247
00:13:17,220 --> 00:13:20,358
we have to multiply all of those weights
我们必须把所有的权重乘

248
00:13:20,358 --> 00:13:23,611
by all of the neurons values at each layer.
以每层的所有神经元值。

249
00:13:23,984 --> 00:13:25,749
And we have to do that again and again
对于每个重量，我们必须

250
00:13:25,749 --> 00:13:27,239
once for each weight.
一次又一次地这样做。

251
00:13:28,264 --> 00:13:32,075
This takes forever in computing terms
在计算规模的计算术语中，这需要

252
00:13:32,075 --> 00:13:33,475
on computing scale.
永远的时间。

253
00:13:33,475 --> 00:13:35,633
And so it's not a practical way
所以这不是一个训练大型神

254
00:13:35,633 --> 00:13:37,351
to train a big neural network.
经网络的实用方法。

255
00:13:38,709 --> 00:13:41,441
You can imagine instead of just rolling down
你可以想象，我们不仅仅是滚到一个简

256
00:13:41,441 --> 00:13:43,253
to the bottom of a simple valley
单的山谷的底部，我们还

257
00:13:43,253 --> 00:13:45,809
we have a very high dimensional valley
有一个非常高维度的山谷，我们必须

258
00:13:45,809 --> 00:13:47,400
and we have to find our way down.
找到我们的方式下降。

259
00:13:47,709 --> 00:13:49,817
And because there are so many dimensions
因为每个权重都有

260
00:13:49,817 --> 00:13:51,386
one for each of these weights
如此多的维度

261
00:13:51,386 --> 00:13:55,021
that the computation just becomes prohibitively expensive.
，以至于计算过程变得非常昂贵。

262
00:13:57,160 --> 00:13:59,822
Luckily there was an insight
幸运的是有一个洞察力可以

263
00:13:59,822 --> 00:14:03,298
that lets us do this in a very reasonable time,
让我们在合理的时间内做到这一点,

264
00:14:03,298 --> 00:14:06,376
and that's that if we're careful about
如果我们仔细设计我们的神

265
00:14:06,376 --> 00:14:07,838
how we design our neural network
经网络，我们

266
00:14:07,838 --> 00:14:10,811
we can calculate this slope directly.
可以直接计算这个斜率。

267
00:14:10,811 --> 00:14:11,610
The gradient.
梯度。

268
00:14:11,610 --> 00:14:13,674
We can figure out the direction
我们可以找出我们需要

269
00:14:13,674 --> 00:14:15,468
that we need to adjust the weight
调整重量的方向，

270
00:14:15,700 --> 00:14:17,550
without going all the way back
而不需要通过我们的

271
00:14:17,550 --> 00:14:19,656
through our neural network and recalculating.
神经网络重新计算。

272
00:14:22,180 --> 00:14:24,220
So just review,
所以回顾一下,

273
00:14:24,220 --> 00:14:26,726
the slope that we're talking about is
我们所说的斜率是

274
00:14:26,726 --> 00:14:28,601
when we make a change in weight
当我们改变重量的时候

275
00:14:28,601 --> 00:14:30,605
the error will change a little bit
错误会改变一点点

276
00:14:31,180 --> 00:14:34,264
and that relation of the change in weight
重量变化的关系

277
00:14:34,264 --> 00:14:37,378
to the change in error is the slope.
误差的变化是斜率。

278
00:14:39,130 --> 00:14:41,368
Mathematically there are several ways to write this.
数学上有几种写这个的方法。

279
00:14:41,589 --> 00:14:43,476
I will favor the one on the bottom.
我喜欢底下那个。

280
00:14:43,476 --> 00:14:45,134
It's technically most correct.
这在技术上是最正确的。

281
00:14:45,134 --> 00:14:48,543
We'll call it DE/DW for shorthand.
我们将它简称为 de / dw。

282
00:14:48,543 --> 00:14:51,391
Every time you see it, just think:
每次你看到它的时候，就想: “

283
00:14:51,987 --> 00:14:55,520
"The change in error when I change a weight"
当我改变一个重量的时候，错误的改变”

284
00:14:55,630 --> 00:14:57,819
Or the change in the thing on the top
或者是顶部的变化，当我

285
00:14:57,819 --> 00:14:59,629
when I change the thing on the bottom.
改变底部的变化时。

286
00:15:00,880 --> 00:15:05,255
This does get us into a little bit of calculus,
这确实让我们进入了一点微积分,

287
00:15:05,255 --> 00:15:06,351
we do take derivatives.
我们求导数。

288
00:15:06,351 --> 00:15:08,399
It's how we calculate slope.
这是我们计算斜率的方法。

289
00:15:08,399 --> 00:15:09,822
If it's new to you
如果你还没学过

290
00:15:09,822 --> 00:15:13,609
I strongly recommend a good semester of calculus.
，我强烈推荐你学一个好学期的微积分。

291
00:15:13,609 --> 00:15:16,905
Just because the concepts are so universal
只是因为这些概念是如此普遍，

292
00:15:16,905 --> 00:15:20,616
and a lot of them have very nice physical interpretations,
而且许多概念都有非常好的物理解释,

293
00:15:20,616 --> 00:15:21,839
which I find very appealing.
我觉得很有吸引力。

294
00:15:22,381 --> 00:15:23,822
But don't worry
但不要担心，

295
00:15:23,822 --> 00:15:26,082
otherwise just gloss over this
否则只是掩盖这一点，

296
00:15:26,082 --> 00:15:27,457
and pay attention to the rest,
注意其余的,

297
00:15:27,457 --> 00:15:29,530
and you'll get a general sense for how this works.
你就能大致了解这是怎么回事了。

298
00:15:31,240 --> 00:15:34,751
So in this case if we change the weight by +1,
在这种情况下，如果我们把权重改为 + 1,

299
00:15:34,751 --> 00:15:36,503
the error changes by -2,
误差变化为 -2,

300
00:15:36,503 --> 00:15:39,102
which gives us a slope of minus two.
这样就得到了负2的斜率。

301
00:15:39,340 --> 00:15:41,806
That tells us the direction
这告诉我们，我们应该调整我

302
00:15:41,806 --> 00:15:43,372
that we should adjust our weight
们的重量的方向，

303
00:15:43,372 --> 00:15:45,008
and how much we should adjust it
以及我们应该调整多少，

304
00:15:45,008 --> 00:15:46,352
to bring the error down.
以减少误差。

305
00:15:48,910 --> 00:15:50,550
Now to do this you have to know
要做到这一点，你必须知道

306
00:15:50,550 --> 00:15:51,914
what your error function is.
你的错误函数是什么。

307
00:15:52,406 --> 00:15:54,948
So assume we had error function
假设我们有误差函数，

308
00:15:54,948 --> 00:15:56,721
that was the square of the weight.
就是权重的平方。

309
00:15:57,320 --> 00:16:01,326
And you can see that our weight is right at -1.
你们可以看到我们的重量正好是 -1。

310
00:16:02,479 --> 00:16:04,690
So the first thing we do is we take the derivative,
我们要做的第一件事就是求导,

311
00:16:04,942 --> 00:16:08,651
change in error divided by change in weight, DE/DW.
误差变化除以权重变化，de / dw。

312
00:16:08,651 --> 00:16:12,290
The derivative of weight squared is two times the weight
重量平方的导数等于重量的2倍，所以我

313
00:16:12,843 --> 00:16:15,031
and so we plug in our weight of -1
们把我们的重量加进去，

314
00:16:15,031 --> 00:16:19,133
and we get a slope DE/DW of minus two.
得到斜率 de / dw 等于 -2。

315
00:16:21,820 --> 00:16:23,063
Now the other trick
现在，另一个让

316
00:16:23,063 --> 00:16:25,588
that lets us do this with deep neural networks
我们用深层神经网络做到这一点

317
00:16:25,588 --> 00:16:26,832
is chaining.
的技巧是链接。

318
00:16:26,832 --> 00:16:28,714
And to show you how this works
为了展示它是如何工

319
00:16:28,780 --> 00:16:31,619
imagine a very simple trivial neural network
作的，想象一个只有一个隐藏层

320
00:16:31,619 --> 00:16:33,662
with just one hidden layer,
的非常简单的神经网络,

321
00:16:33,662 --> 00:16:35,986
one input layer, one output layer
一个输入层，一个输出

322
00:16:35,986 --> 00:16:38,394
and one weight connecting each of them.
层和一个权重连接它们。

323
00:16:39,100 --> 00:16:40,859
So it's obvious to see
很明显，y 

324
00:16:40,859 --> 00:16:42,332
that the value Y
的值就是 

325
00:16:42,332 --> 00:16:45,836
is just the value X times the weight connecting them.
x 乘以连接它们的权重。

326
00:16:45,836 --> 00:16:46,953
W1.
W1.

327
00:16:48,370 --> 00:16:50,782
So if we change W1 a little bit,
所以如果我们稍微改变一下 W1,

328
00:16:50,782 --> 00:16:53,771
we just take the derivative of Y with respect to W1,
我们只需求 y 对 W1的导数,

329
00:16:53,771 --> 00:16:56,339
that we get X, the slope is X.
我们得到 x，斜率是 x。

330
00:16:56,339 --> 00:16:58,569
If I change W1 by a little bit
如果我把 W1改变一点点

331
00:16:58,569 --> 00:17:03,021
then Y will change by X times the size of that adjustment.
，那么 y 就会改变 x 乘以那个调整的大小。

332
00:17:04,540 --> 00:17:06,015
Similarly for the next step.
下一步也是如此。

333
00:17:06,015 --> 00:17:07,390
You can see that E
你可以看到 

334
00:17:07,390 --> 00:17:11,228
is just the value Y times the weight W2.
e 就是 y 乘以重量 W2。

335
00:17:11,800 --> 00:17:16,090
And so when we calculate DE/DY it's just W2.
所以当我们计算 de / dy 时，它只是 W2。

336
00:17:17,933 --> 00:17:19,675
Because this network is so simple
因为这个网络非常简单

337
00:17:19,675 --> 00:17:21,940
we can calculate from one end to the other
，我们可以从一端计算到另一端

338
00:17:22,368 --> 00:17:27,179
X times W1 times W2 is the error E.
X 乘以 W1乘以 W2是误差 e。

339
00:17:27,700 --> 00:17:29,679
And so if we want to calculate
所以如果我们想计算，如果

340
00:17:29,679 --> 00:17:32,420
how much will the error change if I change W1,
我改变 W1，误差会有多大变化,

341
00:17:32,420 --> 00:17:35,599
we just take the derivative of that with respect to W1
我们对 W1求导，得到 

342
00:17:35,599 --> 00:17:37,823
and get X times W2.
x 乘以 W2。

343
00:17:38,680 --> 00:17:41,529
So this illustrates, you can see here now,
这说明，你现在可以看到,

344
00:17:41,529 --> 00:17:43,227
that what we just calculated
我们刚才计

345
00:17:43,227 --> 00:17:47,819
is actually the product of our first derivative
算的实际上是我们一阶导数的

346
00:17:47,819 --> 00:17:48,734
that we took,
乘积,

347
00:17:48,734 --> 00:17:50,656
DY/DW1,
dy / dw1,

348
00:17:50,656 --> 00:17:54,127
times the derivative for the next step,
乘以下一步的导数,

349
00:17:54,127 --> 00:17:55,429
the DE/DY,
de / dy,

350
00:17:55,429 --> 00:17:57,164
multiplied together.
相乘。

351
00:17:59,200 --> 00:18:01,246
This is chaining.
这就是链接。

352
00:18:01,480 --> 00:18:04,524
You can calculate the slope of each tiny step
你可以计算每一个小步骤的

353
00:18:04,524 --> 00:18:07,770
and then multiply all of those together
斜率，然后把所有的斜率相

354
00:18:07,770 --> 00:18:10,866
to get the slope of the full chain...
乘得到整个链的斜率..。

355
00:18:10,866 --> 00:18:12,375
the derivative of the full chain.
完整链的导数。

356
00:18:13,179 --> 00:18:15,285
So in a deeper neural network
所以在更深层的神

357
00:18:15,285 --> 00:18:17,380
what this would look like is
经网络中，如果我

358
00:18:17,380 --> 00:18:18,265
if I want to know
想知道

359
00:18:18,265 --> 00:18:20,193
how much the error will change
如果我调整网络

360
00:18:20,193 --> 00:18:23,518
if I adjust a weight that's deep in the network,
深处的权重，错误会改变多少,

361
00:18:23,518 --> 00:18:28,207
I just calculate the derivative of each tiny little step.
我只是计算每一小步的导数。

362
00:18:28,207 --> 00:18:29,709
All the way back
回到我正在

363
00:18:29,709 --> 00:18:31,807
to the weight that I'm trying to calculate.
计算的重量。

364
00:18:31,807 --> 00:18:34,015
And then multiply them all together.
然后把它们放在一起。

365
00:18:35,361 --> 00:18:38,952
This computationally is many many times cheaper
这种计算方法比我们在重新计算

366
00:18:38,952 --> 00:18:40,796
than what we had to do before
每个重量的整个

367
00:18:40,796 --> 00:18:43,727
of recalculating the error for the whole neural network
神经网络的误差之前要便

368
00:18:43,727 --> 00:18:44,906
for every weight.
宜很多倍。

369
00:18:47,020 --> 00:18:49,894
Now, in the neural network that we've created
现在，在我们创建的神经网

370
00:18:49,894 --> 00:18:52,721
there are several types of
络中，有几种类型的反向传

371
00:18:52,721 --> 00:18:54,222
back propagation we have to do.
播我们必须做。

372
00:18:54,222 --> 00:18:56,313
There are several operations we have to do.
我们还有几个手术要做。

373
00:18:56,313 --> 00:18:57,909
For each one of those
对于其中的每一个

374
00:18:57,909 --> 00:19:00,258
we have to be able to calculate the slope.
，我们必须能够计算出斜率。

375
00:19:00,520 --> 00:19:03,602
So for the first one is just a weighted connection
所以对于第一个，只是两个神经元 a

376
00:19:03,602 --> 00:19:06,034
between two neurons A and B.
 和 b 之间的加权连接。

377
00:19:06,940 --> 00:19:09,226
So let's assume we know
假设我们知道 b

378
00:19:09,226 --> 00:19:11,477
the change in error with respect to B.
 的误差变化。

379
00:19:11,530 --> 00:19:13,081
We want to know
我们想知道 

380
00:19:13,081 --> 00:19:15,325
the change in error with respect to A.
a 的误差变化。

381
00:19:15,325 --> 00:19:19,215
To get there we need to know DB/DA.
为了达到这个目标，我们需要了解 db / da。

382
00:19:20,290 --> 00:19:21,975
So to get that we just write
为了得到这个结果，我们只

383
00:19:21,975 --> 00:19:24,332
the relationship between B and A.
需要写 b 和 a 之间的关系。

384
00:19:24,332 --> 00:19:26,725
Take the derivative of B with respect to A
求 b 关于 a 的导数，

385
00:19:26,725 --> 00:19:28,650
you get the weight W
得到重量 w，

386
00:19:28,650 --> 00:19:31,088
and now we know how to make that step.
现在我们知道怎么做了。

387
00:19:31,088 --> 00:19:33,534
We know how to do that little nugget
我们知道如何做这个小小

388
00:19:33,534 --> 00:19:34,930
of back propagation.
的反向传播。

389
00:19:36,400 --> 00:19:39,548
Another element that we've seen is sums.
我们看到的另一个元素是求和。

390
00:19:39,548 --> 00:19:42,665
All of our neurons sum up a lot of inputs.
我们所有的神经元总结了大量的输入信息。

391
00:19:43,473 --> 00:19:46,251
To take this back propagation step
为了执行这个反向传播步骤，

392
00:19:46,251 --> 00:19:47,712
we do the same thing.
我们做同样的事情。

393
00:19:47,712 --> 00:19:49,201
We write our expression
我们写出

394
00:19:50,515 --> 00:19:54,911
and then we take the derivative of our endpoint Z
表达式，然后对端点 z 对

395
00:19:54,911 --> 00:19:57,030
with respect to a step
传播到 a 

396
00:19:57,030 --> 00:19:58,917
that we are propagating to A.
的步骤求导。

397
00:19:58,917 --> 00:20:02,037
And DZ/DA in this case is just 1.
在这种情况下，dz / da 是1。

398
00:20:02,500 --> 00:20:03,727
Which make sense
如果我们

399
00:20:03,727 --> 00:20:06,588
if we have a sum of a whole bunch of elements,
有一堆元素的总和,

400
00:20:06,935 --> 00:20:09,540
we increase one of those elements by one,
我们将其中一种元素增加一倍,

401
00:20:09,998 --> 00:20:12,139
we expect the sum to increase by one.
我们预计总和会增加1。

402
00:20:12,841 --> 00:20:15,952
That's the definition of a slope of one.
这就是一个斜率的定义。

403
00:20:16,330 --> 00:20:18,011
One-to-one relation there.
一对一的关系。

404
00:20:20,290 --> 00:20:21,953
Another element that we have,
我们还有另外一个元素,

405
00:20:21,953 --> 00:20:23,757
that we need to be able to back propagate
我们需要能够反向繁

406
00:20:23,757 --> 00:20:25,311
is the sigmoid function.
殖的 S形函数。

407
00:20:26,350 --> 00:20:29,816
So this one's a little bit more interesting mathematically.
所以这个在数学上更有趣一点。

408
00:20:30,413 --> 00:20:33,693
I'll just write it shorthand like this, the sigma function.
我会把它写成这样的速记，sigma 函数。

409
00:20:34,300 --> 00:20:38,091
It is entirely feasible to go through
这是完全可行的经过，并采

410
00:20:38,091 --> 00:20:40,302
and take the derivative of this analytically
取导数的这个分

411
00:20:40,302 --> 00:20:42,270
and calculate it.
析和计算它。

412
00:20:42,999 --> 00:20:47,565
It just so happens that this function has a nice property
这个函数有一个很好的性质，要得到

413
00:20:47,565 --> 00:20:49,641
that to get its derivative
它的导数，你只

414
00:20:49,641 --> 00:20:52,983
you just multiply it by one minus itself.
需要把它乘以1减去它本身。

415
00:20:53,590 --> 00:20:57,112
So this is very straightforward to calculate.
所以这是非常简单的计算。

416
00:21:00,160 --> 00:21:01,673
Another element that we've used
我们使用的另一个元

417
00:21:01,673 --> 00:21:03,360
is the rectified linear unit.
素是整流线性单位。

418
00:21:03,700 --> 00:21:06,315
Again to figure out how to back propagate this
为了找到反向传播的方法，

419
00:21:06,315 --> 00:21:07,980
we just write out the relation
我们只要写出，如果

420
00:21:07,980 --> 00:21:12,609
B is equal to A if a is positive, otherwise it's zero.
 a 是正的，关系式 b 等于 a，否则为零。

421
00:21:13,363 --> 00:21:17,526
And piecewise for each of those we take the derivative.
对于每一个，我们求导。

422
00:21:17,526 --> 00:21:22,113
So DB/DA is either one if a is positive or zero.
所以如果 a 为正或者为零，db / da 就是1。

423
00:21:24,700 --> 00:21:26,341
And so with all of these
所以所有这些

424
00:21:26,341 --> 00:21:28,645
little back propagation steps
小小的反向传播步骤

425
00:21:28,645 --> 00:21:31,181
and the ability to chain them together
以及将它们连接在一起的能力

426
00:21:31,809 --> 00:21:33,886
we can calculate the effect
我们可以计算出影响

427
00:21:33,886 --> 00:21:38,392
of adjusting any given weight on the error
调整任何给定权重的错误

428
00:21:38,392 --> 00:21:40,708
for any given input.
任何给定的输入。

429
00:21:40,708 --> 00:21:44,325
And so to train then.
然后就开始训练。

430
00:21:44,829 --> 00:21:47,530
We start with a fully connected network.
我们从一个完全连接的网络开始。

431
00:21:47,530 --> 00:21:50,330
We don't know what any of these weights should be.
我们不知道这些重量应该是多少。

432
00:21:51,309 --> 00:21:53,703
And so we assign them all random values.
所以我们给它们分配所有的随机值。

433
00:21:53,703 --> 00:21:57,911
We create a completely arbitrary random neural network.
我们创建了一个完全任意的随机神经网络。

434
00:21:58,300 --> 00:21:59,949
We put in an input
我们输入我们

435
00:21:59,949 --> 00:22:02,059
that we know the answer to.
知道答案的信息。

436
00:22:02,061 --> 00:22:03,750
We know whether it's solid,
我们知道它是否是固体,

437
00:22:03,750 --> 00:22:05,647
vertical, diagonal or horizontal.
垂直的，对角的或者水平的。

438
00:22:05,647 --> 00:22:07,799
So we know what truth should be
所以我们知道真理应该是什么

439
00:22:07,799 --> 00:22:09,746
and so we can calculate the error.
，所以我们可以计算错误。

440
00:22:10,870 --> 00:22:11,890
Then...
然后..。

441
00:22:12,470 --> 00:22:14,725
we run it through calculate the error
我们运行它通过计算误

442
00:22:14,725 --> 00:22:15,781
and using back propagation
差和使用反

443
00:22:16,580 --> 00:22:19,737
go through and adjust all of those weights
向传播通过，并调整所有这些权

444
00:22:19,737 --> 00:22:22,333
a tiny bit in the right direction.
重一点点在正确的方向。

445
00:22:23,250 --> 00:22:24,560
And then we do that again
然后我们用另一个输

446
00:22:24,560 --> 00:22:25,441
with another input,
入再做一次,

447
00:22:25,441 --> 00:22:27,846
and again with another input for...
还有另外一个输入..。

448
00:22:27,846 --> 00:22:29,313
If we can get away with it
如果我们可以逃

449
00:22:29,313 --> 00:22:32,240
many thousands or even millions of times.
避成千上万次，甚至上百万次。

450
00:22:33,210 --> 00:22:37,142
And eventually all of those weights will gravitate,
最终所有的重量都会被吸引,

451
00:22:37,142 --> 00:22:40,547
they'll roll down that many dimensional valley
它们会从多维山谷滚下来，

452
00:22:40,547 --> 00:22:43,683
to a nice low spot in the bottom.
到底部一个很好的低点。

453
00:22:43,740 --> 00:22:45,947
Where it performs really well
它表现得非常好，

454
00:22:45,947 --> 00:22:49,800
and does pretty close to truth on most of the images.
并且在大多数图像上都非常接近真实。

455
00:22:52,110 --> 00:22:53,617
If we're really lucky
如果我们真的很幸运，

456
00:22:53,617 --> 00:22:55,620
it will look like what we started with.
它将看起来像我们开始时的样子。

457
00:22:55,620 --> 00:23:02,023
With intuitively... understandable receptive fields
凭直觉... ... 可以理解的感受域

458
00:23:02,023 --> 00:23:03,279
for those neurons
和一个相

459
00:23:03,279 --> 00:23:06,315
and a relatively sparse representation
对稀疏的表示意味着

460
00:23:06,315 --> 00:23:07,774
meaning that most of the weights
大多数权

461
00:23:07,774 --> 00:23:10,840
are small or close to zero.
重是小或接近零。

462
00:23:10,840 --> 00:23:12,792
That doesn't always turn out that way.
但事实并非总是如此。

463
00:23:12,792 --> 00:23:15,230
But what we aren't guaranteed
但是我们不能保证

464
00:23:15,230 --> 00:23:19,135
is it'll find a pretty good representation of...
它会找到一个很好的代表..。

465
00:23:19,135 --> 00:23:20,759
you know, the best that it can do
你知道，它所能做的

466
00:23:20,970 --> 00:23:24,164
adjusting those weights to get as close as possible
最好的就是调整这些权重，使它尽可能接

467
00:23:24,164 --> 00:23:26,001
to the right answer for all of the inputs.
近所有输入的正确答案。

468
00:23:30,480 --> 00:23:33,061
So what we've covered is just a very basic introduction
所以我们所讨论的只是对神经网络

469
00:23:33,061 --> 00:23:35,573
to the principles behind neural networks.
背后原理的一个非常基本的介绍。

470
00:23:36,029 --> 00:23:37,597
I haven't told you quite enough
我告诉你的还不够多，

471
00:23:37,597 --> 00:23:39,659
to be able to go out and build one of your own.
还不足以让你走出去自己造一个。

472
00:23:39,659 --> 00:23:41,653
But if you're feeling motivated to do so
但是如果你有动力这样做，

473
00:23:41,653 --> 00:23:43,334
I highly encourage it.
我强烈鼓励你这样做。

474
00:23:43,769 --> 00:23:46,913
Here are a few resources that you'll find useful.
这里有一些你会发现有用的资源。

475
00:23:47,130 --> 00:23:49,814
You'll want to go and learn about biased neurons.
你会想去学习有偏见的神经元。

476
00:23:50,730 --> 00:23:52,920
Dropout is a useful training tool.
辍学是一个有用的培训工具。

477
00:23:53,519 --> 00:23:56,682
There are several resources available
有几个可用的资源，从安德

478
00:23:56,682 --> 00:23:58,483
from Andrej Karpathy
烈卡帕西，他是

479
00:23:58,483 --> 00:24:01,197
who is an expert in neural networks
一个专家在神经网络和

480
00:24:01,197 --> 00:24:03,593
and great at teaching about it.
伟大的教学关于它。

481
00:24:04,200 --> 00:24:06,175
Also there's a fantastic article
还有一篇很棒的文章叫做

482
00:24:06,175 --> 00:24:07,898
called "The Black Magic of Deep Learning"
“深度学习的黑魔法” ，

483
00:24:07,898 --> 00:24:12,000
that just has a bunch of practical "from the trenches" tips
里面有一些实用的“来自战壕”的技巧，告诉你

484
00:24:12,000 --> 00:24:13,877
on how to get them working well.
如何让他们更好的工作。

485
00:24:15,750 --> 00:24:17,412
If you found this useful,
如果你觉得这个有用,

486
00:24:17,412 --> 00:24:19,677
I highly encourage you to visit my blog
我强烈鼓励你访问我的博客

487
00:24:19,677 --> 00:24:23,685
and check out several other "how it works" style posts.
，并检查其他几个“它如何工作”的风格文章。

488
00:24:23,685 --> 00:24:27,604
And the links for these slides you can get as well
这些幻灯片的链接你也

489
00:24:27,604 --> 00:24:30,532
to use however you like.
可以随意使用。

490
00:24:30,532 --> 00:24:33,643
There's also link to them down in the comment section.
在下面的评论部分也有他们的链接。

491
00:24:33,999 --> 00:24:36,019
Thanks for listening.
谢谢你的聆听。
